from airflow.decorators import dag, task
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryCreateEmptyDatasetOperator,
    BigQueryInsertJobOperator
)
from datetime import datetime
from google.cloud import storage
import requests
import pandas as pd
import os
from dotenv import load_dotenv
import redis
import pyarrow as pa
import pyarrow.parquet as pq

# Carregar variáveis do arquivo .env
load_dotenv()

# Variáveis de ambiente
BASE_URL = os.getenv("BASE_URL")
API_KEY = os.getenv("API_KEY")
BUCKET_NAME = os.getenv("GCP_BUCKET_NAME")
DATASET_NAME = os.getenv("BIGQUERY_DATASET_NAME")
MEMORSTORE_HOST = os.getenv("MEMORSTORE_HOST")
MEMORSTORE_PORT = int(os.getenv("MEMORSTORE_PORT"))
MEMORSTORE_PASSWORD = os.getenv("MEMORSTORE_PASSWORD")

# Configuração do Redis (Memorystore)
redis_client = redis.StrictRedis(
    host=MEMORSTORE_HOST,
    port=MEMORSTORE_PORT,
    password=MEMORSTORE_PASSWORD,
    decode_responses=True
)

@dag(
    start_date=datetime(2024, 7, 7),
    schedule=None,
    catchup=False,
    tags=['data-lake', 'bigquery', 'etl'],
)
def erp_bi_data_extraction_with_parquet():
    """
    Pipeline de extração de dados de APIs,
    com armazenamento no GCS (formato Parquet) e carregamento no BigQuery.
    """

    @task
    def fetch_and_store_data(endpoint: str, store_id: str, bus_date: str, data_type: str, category: str):
        """
        Consome dados de uma API e armazena no Google Cloud Storage em formato Parquet.
        """
        # Configurar cache key
        cache_key = f"{endpoint.strip('/')}_{store_id}_{bus_date}"
        cached_data = redis_client.get(cache_key)

        if cached_data:
            print(f"Cache hit for {cache_key}")
            data = pd.read_json(cached_data)
        else:
            print(f"Cache miss for {cache_key}, fetching from API...")
            headers = {"Authorization": f"Bearer {API_KEY}"}
            payload = {"busDt": bus_date, "storeId": store_id}

            response = requests.post(f"{BASE_URL}{endpoint}", json=payload, headers=headers)
            response.raise_for_status()
            data = pd.json_normalize(response.json())

            redis_client.setex(cache_key, 3600, data.to_json())

        # Configurar caminho no GCS com base no tipo de dado
        gcs_path = f"data-lake/raw/{category}/{data_type}/{bus_date[:4]}/{bus_date[5:7]}/store_{store_id}/{data_type}_store_{store_id}_{bus_date}.parquet"

        # Converter DataFrame para Parquet e enviar para o GCS
        table = pa.Table.from_pandas(data)
        buffer = pa.BufferOutputStream()
        pq.write_table(table, buffer)

        client = storage.Client()
        bucket = client.bucket(BUCKET_NAME)
        blob = bucket.blob(gcs_path)
        blob.upload_from_string(buffer.getvalue().to_pybytes(), content_type="application/octet-stream")

        return {
            "gcs_path": gcs_path,
            "data_type": data_type,
            "bus_date": bus_date,
            "store_id": store_id
        }

    create_bigquery_dataset = BigQueryCreateEmptyDatasetOperator(
        task_id="create_bigquery_dataset",
        dataset_id=DATASET_NAME,
        exists_ok=True,
    )

    @task
    def process_all_endpoints(bus_date: str):
        """
        Processa todos os endpoints para todas as lojas.
        """
        endpoints = {
            "/bi/getFiscalInvoice": ("fiscal_invoices", "bi"),
            "/res/getGuestChecks": ("guest_checks", "bi"),
            "/org/getChargeBack": ("chargebacks", "bi"),
            "/trans/getTransactions": ("transactions", "bi"),
            "/inv/getCashManagementDetails": ("cash_management", "bi"),
        }
        store_ids = ["001", "002", "003"]

        results = []
        for endpoint, (data_type, category) in endpoints.items():
            for store_id in store_ids:
                result = fetch_and_store_data(endpoint, store_id, bus_date, data_type, category)
                results.append(result)
        return results

    @task
    def upload_to_bigquery(results):
        """
        Carrega os dados do GCS para o BigQuery.
        """
        for result in results:
            BigQueryInsertJobOperator(
                task_id=f"load_{result['data_type']}_{result['store_id']}",
                configuration={
                    "load": {
                        "sourceUris": [f"gs://{BUCKET_NAME}/{result['gcs_path']}"],
                        "destinationTable": {
                            "projectId": "{{ var.value.gcp_project_id }}",
                            "datasetId": DATASET_NAME,
                            "tableId": f"{result['data_type']}_store_{result['store_id']}"
                        },
                        "sourceFormat": "PARQUET",
                        "writeDisposition": "WRITE_APPEND",
                    }
                }
            )

    # Data de operação
    operation_date = "2024-11-01"  # Exemplo de data para o desafio

    # Orquestração
    fetched_data = process_all_endpoints(operation_date)
    upload_to_bigquery(fetched_data) >> create_bigquery_dataset


erp_bi_data_extraction_with_parquet_dag = erp_bi_data_extraction_with_parquet()
